{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bfa583-eb43-4f58-82e7-72d0f440fe5c",
   "metadata": {},
   "source": [
    "# ID2214 Fx Assignment\n",
    "Abyel Tesfay, Abyel@kth.se\n",
    "\n",
    "### Instructions\n",
    "The following jupyter notebook contains solutions to a set of tasks in the form of simulations and tests, explanations and any assumptions made. This notebook was written with the purpose of completing the assignments below. Each assignment consists of an explanation and a form of simulation (or results from it). Below the assignments you will find the code use for data preparation, modelling and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcdc886-6387-4dc8-8b43-88ccd4e69c3d",
   "metadata": {},
   "source": [
    "## Load packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bfd19-b51b-4366-9d9e-4ebaeaeb636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f06f-4481-4625-be63-b20a4b63d9f5",
   "metadata": {},
   "source": [
    "## 1a. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb8478-e1f7-44d5-b3cb-73b31b2339d1",
   "metadata": {},
   "source": [
    "It depends on the outcome of the models generated from the hyper-parameter settings and the algorithm used. The performance of the best-performing model is biased on how the given dataset is randomly split into two samples. Therefore the performance (accuracy) of the best-performing model might be too optimistic, its good score is dependent on the current sample that was randomly generated. For this observation i performed the following steps.\n",
    "- I chose the dataset \"healthcare-dataset-stroke-data.csv\" which is classified with binary labels\n",
    "- I prepared two equally sized samples using randomized sampling\n",
    "- For modelling i used RandomForest with the hyper-parameters 'n_estimators', 'criterion' and 'max_features', the best performing model was picked by the highest average accuracy from a ten-fold cross-validation\n",
    "- For performance estimation i trained a model with the best configuration and a baseline model, using the first half as training set. I then tested both models using the second half as a test set.\n",
    "\n",
    "Using the hyper-parameters 'n_estimators'= [1,10,50,100,250], criterion ['gini', 'entropy'] and 'max_features' = [1,2,...,10] i received the following results:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4c9511f-90b3-4c9a-a081-30d1cea75f53",
   "metadata": {},
   "source": [
    "Modelling & cross-validation: Hyper-parameters is better\n",
    "    best hyper-parameters:  {'trees': 50, 'critera': 'gini', 'features': 2}\n",
    "    hyper-parameters score:  0.954205\n",
    "    base model score:  0.953421\n",
    "    no. hyper-parameters better than baseline model:  41\n",
    "\n",
    "Evaluation: baseline model is better or equal\n",
    "    Accuracy hyper-par: 0.947945 , trees: 50 , criterion: gini , features: 2\n",
    "    Accuracy baseline:  0.948337 , trees 100 , criterion: gini , features: auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a235396-af20-4cb0-836d-cf301d91db32",
   "metadata": {},
   "source": [
    "The results show that even if the best-performing configuration for hyper-parameters (and algorithm) outperforms the baseline model in the first half of data, the baseline model may still *outperform the best-performing configuration* in the second half. I also checked the amount models that performed better than the baseline during modelling, to see if a majority of them could outperform on the first half. If this was true, then the best performing configuration would be *more likely to outperform* the baseline on the second half of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61099f76-2709-48ca-b7ea-cf6f6cd73bcc",
   "metadata": {},
   "source": [
    "## 1b. Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d129a-67cd-446a-a6b1-3f3a9bda5321",
   "metadata": {},
   "source": [
    "Assuming that the model was trained on a imbalanced training set which contains instances that are not present in the test set, we should expect a **lower accuracy but a similar AUC** when evaluating the model on the class-balanced set. The reason is that the model was trained on a imbalanced set where the majority class is frequent. When evaluated on a class-balanced test set (which has a lower frequency of the majority class) the accuracy will decrease. For the AUC however we will see a similar performance. The AUC only measures the probability of the model to rank an instance with the correct label ahead of instances with the wrong label. A lower accuracy will not affect this metric. \n",
    "\n",
    "The following steps were taken with two different datasets\n",
    "- Select a data set for the task\n",
    "- Split the dataset into two halves, one training set and one 'sampling' set \n",
    "- Use the sampling set to create the following test sets described in 1b:\n",
    "    - An imbalanced test set in which the majority class is four times more frequent than the minority class\n",
    "    - A class-balanced test set (has fewer instances than the above data set)\n",
    "- Perform data preparation on the training set: filtering and imputation\n",
    "- Generate and train two identical models using a selected algorithm e.g RandomForest\n",
    "- Evaluate the models using both the imbalanced and balanced test sets\n",
    "\n",
    "Results, smiles_one_hot.csv:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "823b067a-6276-423a-ab04-a4f00fea2461",
   "metadata": {},
   "source": [
    "                Accuracy     AUC\n",
    "Imbalanced       0.80284  0.7687\n",
    "Class-balanced   0.50948  0.7306"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0809c9-9c4b-42b4-ba2d-d3898d754a1c",
   "metadata": {},
   "source": [
    "Results, diabetes_binary_health_indicators_BRFSS2015.csv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6023328d-31ce-4adb-a1e6-07bcdf54b46e",
   "metadata": {},
   "source": [
    "                Accuracy     AUC\n",
    "Imbalanced        0.8128  0.8049\n",
    "class-balanced    0.5725  0.8024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c1fe5-117b-49d8-bde4-b28e0c8d572f",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872a753-2a97-4673-a955-4692a8c658ce",
   "metadata": {},
   "source": [
    "## Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b3c5c-eae8-4131-8374-29d05bb3deb6",
   "metadata": {},
   "source": [
    "### Helper functions required for 1a and 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b067b562-649d-4717-9bf9-8706407194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "def create_column_filter(df):\n",
    "    df2 = df.copy()\n",
    "    column_filter = list(df2.columns)\n",
    "    columns = [col for col in df2.columns if col not in ['active', 'index', 'id', 'class']]\n",
    "    for col in columns:\n",
    "        if df2[col].isnull().all():\n",
    "            df2.drop(columns=col, inplace=True)\n",
    "            column_filter.remove(col)\n",
    "            continue\n",
    "\n",
    "        if len(df2[col].dropna().unique()) <= 1:\n",
    "            df2.drop(columns=col, inplace=True)\n",
    "            column_filter.remove(col)\n",
    "\n",
    "    return df2, column_filter\n",
    "\n",
    "def apply_column_filter(df, column_filter):\n",
    "    df2 = df.copy()\n",
    "    [df2.drop(columns=col, inplace=True) for col in df2.columns if col not in column_filter]\n",
    "    return df2\n",
    "\n",
    "def create_normalization(df, normalizationtype='minmax'):\n",
    "    df2 = df.copy()\n",
    "    include_types = np.int32, np.int64, np.float32, np.float64\n",
    "    columns = [col for col in df2.columns if col not in ['active', 'index', 'id', 'class']\n",
    "               and df2[col].dtype in include_types]\n",
    "    normalization = {}\n",
    "    for col in columns:\n",
    "        if normalizationtype=='minmax':\n",
    "            min = df2[col].min()\n",
    "            max = df2[col].max()\n",
    "            normalization[col] = normalizationtype, min, max\n",
    "        elif normalization=='zscore':\n",
    "            mean = df2[col].mean()\n",
    "            std = df[col].std()\n",
    "            normalization = normalizationtype, mean, std\n",
    "\n",
    "    for col in columns:\n",
    "        values = list(normalization[col])\n",
    "        if values[0] == 'minmax':\n",
    "            df2[col] = [(x-values[1])/(values[2]-values[1]) for x in df[col]]\n",
    "\n",
    "    return df2, normalization\n",
    "\n",
    "def apply_normalization(df, normalization):\n",
    "    df2 = df.copy()\n",
    "    include_types = np.int32, np.int64, np.float32, np.float64\n",
    "    columns = [col for col in df2.columns if col not in ['active', 'index', 'id', 'class']\n",
    "               and df2[col].dtype in include_types]\n",
    "    for col in columns:\n",
    "        values = list(normalization[col])\n",
    "        if values[0] == 'minmax':\n",
    "            df2[col] = [(x-values[1])/(values[2]-values[1]) for x in df[col]]\n",
    "    return df2\n",
    "\n",
    "def create_imputation(df):\n",
    "    df2 = df.copy()\n",
    "    numeric_types = np.int32, np.int64, np.float32, np.float64\n",
    "    columns = [col for col in df2.columns if col not in ['active', 'index', 'id', 'class']]\n",
    "    imputation = {}\n",
    "    for col in columns:\n",
    "        if df2[col].dtype in numeric_types:\n",
    "            if df2[col].isnull().all():\n",
    "                df2[col].fillna(0, inplace=True)\n",
    "            imputation[col] = df2[col].mean()\n",
    "            df2[col].fillna(df2[col].mean(), inplace=True)\n",
    "        else:\n",
    "            if df2[col].isnull().all():\n",
    "                df2[col].fillna('', inplace=True) if df2[col].dtype == 'object' else \\\n",
    "                    df2[col].astype('category') and df2[col].fillna(df2[col].cat.categories[0], inplace=True)\n",
    "\n",
    "            imputation[col] = df2[col].mode()[0]\n",
    "            df2[col].fillna(imputation[col], inplace=True)\n",
    "\n",
    "    return df2, imputation\n",
    "\n",
    "def apply_imputation(df, imputation):\n",
    "    df2 = df.copy()\n",
    "    return df2.fillna(value=imputation)\n",
    "\n",
    "def create_one_hot(df):\n",
    "    df2=df.copy()\n",
    "    columns = [col for col in df2.columns if col not in ['active', 'index', 'id', 'class']]\n",
    "    one_hot={}\n",
    "    for col in columns:\n",
    "        if df2[col].dtype.name != 'category' and df2[col].dtype.name != 'object':\n",
    "            continue\n",
    "        one_hot[col]=df2[col].unique()\n",
    "        tmp = pd.get_dummies(df2[col], prefix=col, prefix_sep='-', dtype=np.float64)\n",
    "        df2.drop(columns=col, inplace=True)\n",
    "        df2 = pd.concat([df2, tmp], axis=1)\n",
    "\n",
    "    return df2, one_hot\n",
    "\n",
    "def apply_one_hot(df, one_hot):\n",
    "    new_df = df.copy()\n",
    "    for e in new_df.columns:\n",
    "        if e in one_hot:\n",
    "            for i in one_hot[e]: \n",
    "                new_df[e + \"-\" + i] = [1.0 if x == i else 0.0 for x in new_df[e]]\n",
    "                new_df[e + \"-\" + i].astype('float')                                     \n",
    "            new_df.drop(e, axis=1, inplace=True)                                             \n",
    "    return new_df\n",
    "\n",
    "def accuracy(df, correctlabels):\n",
    "    highest_probability = df.idxmax(axis=1)\n",
    "    correct_occurances = 0\n",
    "    for correct_label, predicted_label in zip(correctlabels, highest_probability):\n",
    "        if correct_label==predicted_label:\n",
    "            correct_occurances+=1\n",
    "\n",
    "    return correct_occurances/df.index.size\n",
    "\n",
    "def brier_score(df, correctlabels):\n",
    "    squared_sum = 0\n",
    "    row = 0\n",
    "    for label in correctlabels:\n",
    "        i = np.where(df.columns==label)[0]\n",
    "        for col in df.columns:\n",
    "            squared_sum += (1 - df.loc[row, label])**2 if label==col else df.loc[row, col]**2\n",
    "        row+=1\n",
    "\n",
    "    return squared_sum/df.index.size\n",
    "\n",
    "def auc(df, correctlabels):\n",
    "    auc=0\n",
    "    for col in df.columns:\n",
    "        df2 = pd.concat([df[col], pd.Series(correctlabels.astype('category'), name='correct')], axis=1)\n",
    "        # get dummies for correct labels and sort descending\n",
    "        df2 = pd.get_dummies(df2.sort_values(col, ascending=False))\n",
    "\n",
    "        # move col to first for easier total tp and fp calculation\n",
    "        tmp=df2.pop('correct_'+str(col))\n",
    "        # get the col frequency for calculating weighted AUCs\n",
    "        col_frequency=tmp.sum()/tmp.index.size\n",
    "        df2.insert(1, tmp.name, tmp)\n",
    "        scores={}\n",
    "        # populate the scores dictionary for column i.e. key=score, value=[tp_sum, fp_sum]\n",
    "        for row in df.index:\n",
    "            key=df2.iloc[row, 0]\n",
    "            current=np.zeros(2, dtype=np.uint) if scores.get(key) is None else scores[key]\n",
    "            to_add=np.array([1,0]) if df2.iloc[row, 1]==1 else np.array([0,1])\n",
    "            scores[key]=current+to_add\n",
    "\n",
    "        # calculate auc based on scores\n",
    "        cov_tp=0\n",
    "        column_auc=0\n",
    "        tot_tp=0\n",
    "        tot_fp=0\n",
    "        # calculate total tp and fp\n",
    "        for value in scores.values():\n",
    "            tot_tp+=int(value[0])\n",
    "            tot_fp+=int(value[1])\n",
    "\n",
    "        # same algorithm as in the lecture \n",
    "        for i in scores.values():\n",
    "            if i[1] == 0:\n",
    "                cov_tp+=i[0]\n",
    "            elif i[0] == 0:\n",
    "                column_auc += (cov_tp/tot_tp)*(i[1]/tot_fp)\n",
    "            else:\n",
    "                column_auc += (cov_tp/tot_tp)*(i[1]/tot_fp)+(i[0]/tot_tp)*(i[1]/tot_fp)/2\n",
    "                cov_tp += i[0]\n",
    "\n",
    "        auc+=col_frequency*column_auc\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c7f6e-bc77-4109-98a7-ef4bab978568",
   "metadata": {},
   "source": [
    "### Task 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e211134-933f-4537-bc9b-130f73b07c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation for task 1a\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "####################### - Helper functions\n",
    "\"\"\"\n",
    "Splits the data into two random, equally-sized data sets. Compared to the\n",
    "above, this function uses random sampling of the data set\n",
    "\"\"\"\n",
    "def random_split(df, Class_name):\n",
    "    df1 = df.copy()\n",
    "    df1_reshuffled = np.random.choice(df1.index, len(df1.index), replace=False)\n",
    "    \n",
    "    # create a list of two samples, both are used to create train and test sets\n",
    "    two_random_indexSets = np.random.choice(df1_reshuffled, (2,int(len(df1.index)/2)), replace=False)\n",
    "\n",
    "    # list for both datasets\n",
    "    df_list = []\n",
    "    \n",
    "    for elem in two_random_indexSets:\n",
    "        df2 = df1.iloc[elem]      \n",
    "        df2_reshuffled_indexes = np.random.choice(len(df2),len(df2),replace=False)\n",
    "        data_set = [df2.iloc[i] for i in df2_reshuffled_indexes]\n",
    "        df3 = pd.DataFrame(data_set, columns = list(df1))\n",
    "        df3.index = range(len(df3.index))\n",
    "        df_list.append(df3)\n",
    "\n",
    "    # determine through coin flip which set that becomes the test set and training set\n",
    "    flip = np.random.choice(len(df_list), 1, replace=False)\n",
    "    test = df_list[flip[0]]\n",
    "    df_list.pop(flip[0])\n",
    "    training = df_list[0]\n",
    "    \n",
    "    return training, test\n",
    "\n",
    "####################### - Main\n",
    "\n",
    "#input here!\n",
    "dataset_name = \"healthcare-dataset-stroke-data.csv\"\n",
    "Class_label_name = \"stroke\" # set class label name here\n",
    "print(\"Data set: \" + dataset_name + \", class label: \" + Class_label_name)\n",
    "print()\n",
    "\n",
    "# Get the dataset\n",
    "data_set = pd.read_csv(dataset_name)\n",
    "ON_data_set = data_set.copy()\n",
    "\n",
    "# Check the amount classes between the two\n",
    "class1 = sum(ON_data_set[Class_label_name].values == 0)\n",
    "class2 = sum(ON_data_set[Class_label_name].values == 1)\n",
    "print(\"Amount instances with the following classes\")\n",
    "print(\"0: \" + str(class1))\n",
    "print(\"1: \" + str(class2))\n",
    "\n",
    "# the amount features\n",
    "features = len(ON_data_set.columns)\n",
    "print(\"Amount features: \" + str(features))\n",
    "print()\n",
    "\n",
    "# Split into two sets, one for cross-validation and one for testing (evaluation)\n",
    "training_set, test_set = random_split(ON_data_set, Class_label_name)\n",
    "\n",
    "## distribution\n",
    "train_class0 = sum(training_set[Class_label_name].values == 0)\n",
    "train_class1 = sum(training_set[Class_label_name].values == 1)\n",
    "test_class0 = sum(test_set[Class_label_name].values == 0)\n",
    "test_class1 = sum(test_set[Class_label_name].values == 1)\n",
    "\n",
    "print(\"Class distribution\")\n",
    "print(\"training set --- 0: \" + str(train_class0) + \", 1: \" + str(train_class1))\n",
    "print(\"test set --- 0: \" + str(test_class0) + \", 1: \" + str(test_class1))\n",
    "\n",
    "# Save into csv files\n",
    "training_set.to_csv(\"training_set.csv\", index=False)\n",
    "test_set.to_csv(\"test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d517a-996a-4996-9053-aef68f132389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling for task 1a\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "############# Init\n",
    "\n",
    "# input here!\n",
    "np.random.seed(100) # Pick the same seed, for random generation\n",
    "Class_label_name = 'stroke' # change edepending on class name\n",
    "Total_features = 12         # amount features\n",
    "\n",
    "# Get training set\n",
    "training_set = pd.read_csv(\"training_set.csv\")\n",
    "\n",
    "# Get the instances (X)\n",
    "X_rf = training_set.copy()\n",
    "\n",
    "# Get the labels (Y), drop then in the training set\n",
    "Y = training_set[Class_label_name].astype('category')\n",
    "X_rf.drop(columns=[Class_label_name], inplace=True)\n",
    "\n",
    "############# Data preparation\n",
    "\n",
    "# RF\n",
    "X_rf, column_filter = create_column_filter(X_rf)\n",
    "X_rf, imputation = create_imputation(X_rf)\n",
    "X_rf, one_hot = create_one_hot(X_rf)\n",
    "\n",
    "# drop unecessary columns (for certain datasets)\n",
    "# X_rf = X_rf.drop(['id'], axis=1)\n",
    "\n",
    "############# Modelling\n",
    "\n",
    "# Prepare cross-validation\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Prepare hyper-parameters\n",
    "\n",
    "num_trees = [1,10,50,100,250]\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "max_f = range(Total_features + len(one_hot))[1: 11]\n",
    "\n",
    "scores = []\n",
    "hyper_parameters = []\n",
    "\n",
    "# Do cross-validation for RF\n",
    "for num in num_trees:\n",
    "    for crit in criterion:\n",
    "        for no_featues in max_f:\n",
    "            model = RandomForestClassifier(n_estimators=num, criterion=crit, max_features=no_featues)\n",
    "            new_sc = np.mean(cross_val_score(model, X_rf, Y, scoring=\"accuracy\", cv=cv, n_jobs=-1))\n",
    "            hyper_parameters.append({\"trees\": num, \"critera\": crit, \"features\": no_featues})\n",
    "            scores.append(new_sc)\n",
    "        \n",
    "# Find the best performing model based on score (and the best configuration)\n",
    "indx = np.argmax(scores)\n",
    "best_parameters = hyper_parameters[indx]\n",
    "    \n",
    "# Create a baseline model & validate it\n",
    "model = RandomForestClassifier()\n",
    "base_sc = np.mean(cross_val_score(model, X_rf, Y, scoring=\"accuracy\", cv=cv, n_jobs=-1))\n",
    "\n",
    "# Optional data: check the amount hyper-parameter setting that gives better accuracy than the baseline\n",
    "better_than_base = sum(scores > base_sc)\n",
    "\n",
    "print(\"Modelling & cross-validation:\")\n",
    "if scores[indx] > base_sc:\n",
    "    print('Hyper-parameters is better')\n",
    "else:\n",
    "    print('baseline model is better or equal')\n",
    "print('best hyper-parameters: ', best_parameters)\n",
    "print('hyper-parameters score: ', round(scores[indx], 6))\n",
    "print('base model score: ', round(base_sc, 6))\n",
    "print('no. hyper-parameters better than baseline model: ', better_than_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005996fa-2188-4bf8-b81d-7c600cce062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation for task 1a\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "############# Start\n",
    "\n",
    "# Set the seed and the best hyper-parameters\n",
    "np.random.seed(100)\n",
    "no_trees, criteria, no_features = 50, \"gini\", 2\n",
    "Class_label_name = 'stroke'\n",
    "\n",
    "# Get both training and test sets\n",
    "test_set = pd.read_csv(\"test_set.csv\")\n",
    "train_set = pd.read_csv(\"training_set.csv\")\n",
    "\n",
    "# Create the X and Y datasets for training the models\n",
    "X_train = train_set.copy()\n",
    "Y_train = X_train[Class_label_name].astype('category')\n",
    "\n",
    "############# Data preparation\n",
    "\n",
    "X_train.drop(columns=[Class_label_name], inplace=True)\n",
    "X_train, column_filter = create_column_filter(X_train)\n",
    "X_train, imputation = create_imputation(X_train)\n",
    "X_train, one_hot = create_one_hot(X_train)\n",
    "\n",
    "# Training the best performing model and baseline model on the train set\n",
    "rf_hyper_parameters = RandomForestClassifier(n_estimators=no_trees, criterion=criteria, max_features=no_features)\n",
    "rf_hyper_parameters.fit(X_train, Y_train)\n",
    "rf_base_line = RandomForestClassifier()\n",
    "rf_base_line.fit(X_train, Y_train)\n",
    "\n",
    "# Prepare the test set, both X and Y \n",
    "results = []\n",
    "X_test = test_set.copy()\n",
    "Y_true = X_test[Class_label_name].astype('category')\n",
    "X_test.drop(columns=[Class_label_name], inplace=True)\n",
    "\n",
    "# Apply data preparation\n",
    "X_test = apply_column_filter(X_test, column_filter)\n",
    "X_test = apply_imputation(X_test, imputation)\n",
    "X_test = apply_one_hot(X_test, one_hot)\n",
    "\n",
    "############# Testing (evaluation)\n",
    "y_pred_hyper = rf_hyper_parameters.predict(X_test)\n",
    "y_pred_baseline = rf_base_line.predict(X_test)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy_hyper_parameters = round(accuracy_score(Y_true, y_pred_hyper), 10)\n",
    "accuracy_base_line = round(accuracy_score(Y_true, y_pred_baseline), 10)\n",
    "results.append([accuracy_hyper_parameters, accuracy_base_line])\n",
    "\n",
    "print()\n",
    "print(\"Evaluation:\")\n",
    "if accuracy_hyper_parameters > accuracy_base_line:\n",
    "    print('Hyper-parameters is better')\n",
    "else:\n",
    "    print('baseline model is better or equal')\n",
    "print('Accuracy hyper-par:', accuracy_hyper_parameters, ', trees:', no_trees, \n",
    "      ', criterion:', criteria ,', features:', no_features)\n",
    "print('Accuracy baseline: ', accuracy_base_line, ', trees', rf_base_line.n_estimators, \n",
    "      ', criterion:', rf_base_line.criterion ,', features:', rf_base_line.max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d4c43-f0c9-44db-a0c4-869d687b47fe",
   "metadata": {},
   "source": [
    "### Task 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f3606-b3ca-4cde-be1c-55a90b4d1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data prearation for 1b\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "####################### - Helper functions\n",
    "\n",
    "\"\"\"\n",
    "Splits the data into two random, equally-sized data sets. Stratified sampling\n",
    "is used to make both samples contains equal probability models to get instances \n",
    "with one of the class labels e.g. \"1\" or \"0\" \n",
    "\"\"\"\n",
    "def data_split(df, Class_name):\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    labels_0_indexes = np.where(df1[Class_name].values == 0)[0]\n",
    "    labels_0_indexes_sets = np.random.choice(labels_0_indexes, (2,int(len(labels_0_indexes)/2)), replace=False)\n",
    "    \n",
    "    labels_1_indexes = np.where(df1[Class_name].values == 1)[0]\n",
    "    labels_1_indexes_sets = np.random.choice(labels_1_indexes, (2,int(len(labels_1_indexes)/2)), replace=False)\n",
    "    \n",
    "    data_sets = zip(labels_1_indexes_sets,labels_0_indexes_sets)\n",
    "    data_sets_list = list(data_sets)\n",
    "\n",
    "    # list for both datasets\n",
    "    df_list = []\n",
    "\n",
    "    # split the dataset into 2 using indexes\n",
    "    for elem in data_sets_list:\n",
    "        df2 = pd.concat([df1.iloc[elem[0]], df1.iloc[elem[1]]])       \n",
    "        df2_reshuffled_indexes = np.random.choice(len(df2),len(df2),replace=False)\n",
    "        data_set = [df2.iloc[i] for i in df2_reshuffled_indexes]\n",
    "        df3 = pd.DataFrame(data_set, columns = list(df1))\n",
    "        df3.index = range(len(df3.index))\n",
    "        df_list.append(df3)\n",
    "        \n",
    "    test = df_list[1]\n",
    "    training = df_list[0]\n",
    "    \n",
    "    return training, test\n",
    "\n",
    "####################### - Data preparation\n",
    "\n",
    "dataset_name = \"smiles_one_hot.csv\"\n",
    "Class_label_name = \"active\" # set class label name here\n",
    "print(\"Data set: \" + dataset_name + \", class label: \" + Class_label_name)\n",
    "print()\n",
    "\n",
    "# Get the dataset, switch here with different datasets\n",
    "data_set = pd.read_csv(dataset_name)\n",
    "ON_data_set = data_set.copy()\n",
    "\n",
    "# Check the amount classes between the two\n",
    "class1 = sum(ON_data_set[Class_label_name].values == 0)\n",
    "class2 = sum(ON_data_set[Class_label_name].values == 1)\n",
    "print(\"Amount instances with the following classes\")\n",
    "print(\"0: \" + str(class1))\n",
    "print(\"1: \" + str(class2))\n",
    "\n",
    "# the amount features\n",
    "features = len(ON_data_set.columns)\n",
    "print(\"Amount features: \" + str(features))\n",
    "print()\n",
    "\n",
    "# Split into two sets, one for cross-validation and one for testing (evaluation)\n",
    "training_set, test_set = data_split(ON_data_set, Class_label_name)\n",
    "\n",
    "## distribution\n",
    "train_class0 = sum(training_set[Class_label_name].values == 0)\n",
    "train_class1 = sum(training_set[Class_label_name].values == 1)\n",
    "test_class0 = sum(test_set[Class_label_name].values == 0)\n",
    "test_class1 = sum(test_set[Class_label_name].values == 1)\n",
    "\n",
    "print(\"Class distribution\")\n",
    "print(\"training set --- 0: \" + str(train_class0) + \", 1: \" + str(train_class1))\n",
    "print(\"test set --- 0: \" + str(test_class0) + \", 1: \" + str(test_class1))\n",
    "\n",
    "# Save into csv\n",
    "training_set.to_csv(\"B_training_set.csv\", index=False)\n",
    "test_set.to_csv(\"B_sampling_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e3f82-e3e8-4334-be5b-3afc7a1ca399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preparation of the majority and equal-sized samples described in 1b\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "####################### - Helper functions\n",
    "\n",
    "## Creates a dataset in which the majority class is equal the minority\n",
    "def equal_sampling(df, classname, Half_Class_label):\n",
    "    df1 = df.copy()\n",
    "    labels_0_indexes = np.where(df[classname].values == 0)[0]\n",
    "    labels_0_indexes_sets = np.random.choice(labels_0_indexes, Half_Class_label, replace=False)       # 211 or 528\n",
    "    \n",
    "    labels_1_indexes = np.where(df1[classname].values == 1)[0]\n",
    "    labels_1_indexes_sets = np.random.choice(labels_1_indexes, Half_Class_label, replace=True)       # 211 or 527\n",
    "    \n",
    "    df2 = pd.concat([df1.iloc[labels_0_indexes_sets], df1.iloc[labels_1_indexes_sets]])       \n",
    "    df2_reshuffled_indexes = np.random.choice(len(df2),len(df2),replace=False)\n",
    "    data_set = [df2.iloc[i] for i in df2_reshuffled_indexes]\n",
    "    df3 = pd.DataFrame(data_set, columns = list(df1))\n",
    "    df3.index = range(len(df3.index))\n",
    "    \n",
    "    return df3\n",
    "\n",
    "## Creates a dataset in which the majority class is represented as 4/5 in the dataset\n",
    "def adjust_sampling(df, classname, Half_Class_label, four_of_five):\n",
    "    df1 = df.copy()\n",
    "    labels_0_indexes = np.where(df1[classname].values == 0)[0]\n",
    "    labels_0_indexes_sets = np.random.choice(labels_0_indexes, four_of_five, replace=False)     # 4/5 of the size\n",
    "    \n",
    "    labels_1_indexes = np.where(df1[classname].values == 1)[0]\n",
    "    labels_1_indexes_sets = np.random.choice(labels_1_indexes, Half_Class_label, replace=True)  # 1/5 of the size\n",
    "\n",
    "    df2 = pd.concat([df1.iloc[labels_0_indexes_sets], df1.iloc[labels_1_indexes_sets]])       \n",
    "    df2_reshuffled_indexes = np.random.choice(len(df2),len(df2),replace=False)\n",
    "    data_set = [df2.iloc[i] for i in df2_reshuffled_indexes]\n",
    "    df3 = pd.DataFrame(data_set, columns = list(df1))\n",
    "    df3.index = range(len(df3.index))\n",
    "    \n",
    "    return df3\n",
    "\n",
    "####################### - Data preparation\n",
    "Class_name = \"active\"  # set class name here\n",
    "Half_Class_label = 211 # hardcoded, represents half the amount of minority class label \n",
    "                        # e.g. 211 in smiles.one_hot, 1000 in diabetes_binary dataset\n",
    "four_of_five = 844     # hardcoded, represents 4/5 of the majority class label \n",
    "                        # e.g. 844 in smiles.one_hot, 4000 in diabetes_binary dataset\n",
    "\n",
    "# Get the test dataset, copy for majority and one for equal_sampling\n",
    "data_set = pd.read_csv(\"B_sampling_set.csv\")\n",
    "major_set = data_set.copy()\n",
    "equal_set = data_set.copy()\n",
    "\n",
    "#Split into two sets, one for training and one for testing\n",
    "major_set = adjust_sampling(major_set, Class_name, Half_Class_label, four_of_five)   # for 5:1 majority set\n",
    "equal_set = equal_sampling(equal_set, Class_name, Half_Class_label)                  # for equal sets\n",
    "\n",
    "#save into csv\n",
    "major_set.to_csv(\"B_majority_test.csv\", index=False)\n",
    "equal_set.to_csv(\"B_undersample_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5b456-9a24-47b0-88a8-56cc38824283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling and testing of the data sets, 1b\n",
    "@author: Abyel\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "################ HELPER FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "Evaluates the given model with test_set. Uses f,i and o to apply \n",
    "filtering, imputation and one_hot encoding\n",
    "\"\"\"\n",
    "def evaluate_model(model, test_set, f, i, o, classname):\n",
    "    X_test = test_set.copy()\n",
    "    Y_true = X_test[classname].astype('category')\n",
    "    X_test.drop(columns=[classname], inplace=True)\n",
    "    \n",
    "    # applying data preparation\n",
    "    X_test = apply_column_filter(X_test, f)\n",
    "    X_test = apply_imputation(X_test, i)\n",
    "    X_test = apply_one_hot(X_test, o)\n",
    "\n",
    "\n",
    "    # get predictions and score with given test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "\n",
    "    # get the AUC and accuracy\n",
    "    accuracy = round(accuracy_score(Y_true, y_pred), 6)\n",
    "    try:\n",
    "        AUC = round(roc_auc_score(Y_true, y_score[:, 1]), 6)\n",
    "    except ValueError:\n",
    "        print(\"ERROR AUC\")\n",
    "        AUC = 0\n",
    "    \n",
    "    return accuracy, AUC\n",
    "\n",
    "################ MAIN\n",
    "\n",
    "# Set seed and no trees, also set the name of the class label\n",
    "np.random.seed(100)\n",
    "no_trees = 100\n",
    "classname = 'active'\n",
    "\n",
    "# get training set and both test sets\n",
    "training_set = pd.read_csv(\"B_training_set.csv\")\n",
    "majority_set = pd.read_csv(\"B_majority_test.csv\")\n",
    "undersample_set = pd.read_csv(\"B_undersample_test.csv\")\n",
    "\n",
    "# get the instances (X)\n",
    "X_train = training_set.copy()\n",
    "Y_train = X_train[classname].astype('category')\n",
    "\n",
    "### Data preparation\n",
    "X_train.drop(columns=[classname, 'index'], inplace=True, errors='ignore')\n",
    "X_train, column_filter = create_column_filter(X_train)\n",
    "X_train, imputation = create_imputation(X_train)\n",
    "X_train, one_hot = create_one_hot(X_train)\n",
    "\n",
    "### Modelling\n",
    "model = RandomForestClassifier(n_estimators=no_trees)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Prepare both test sets and evaluate the AUC and accuracy\n",
    "results = []\n",
    "major_accuracy, major_auc = evaluate_model(model, majority_set, column_filter, imputation, one_hot, classname)\n",
    "under_accuracy, under_auc = evaluate_model(model, undersample_set, column_filter, imputation, one_hot, classname)\n",
    "\n",
    "## print \n",
    "rows = [[major_accuracy, major_auc], [under_accuracy, under_auc]]\n",
    "results_df = pd.DataFrame(rows, columns=['Accuracy', 'AUC'])\n",
    "results_df.index = ['Imbalanced', 'class-balanced']\n",
    "print()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9360724-f341-4def-bc7c-588d437414b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
